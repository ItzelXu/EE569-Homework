{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Classification using SSL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Global Libraries\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from skimage.util import view_as_windows\n",
    "from skimage.measure import block_reduce\n",
    "from keras.utils.np_utils import to_categorical\n",
    "#Import PixelHop Libraries\n",
    "from lag import LAG\n",
    "from llsr import LLSR as myLLSR\n",
    "from pixelhop2 import Pixelhop2\n",
    "from cross_entropy import Cross_Entropy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Image Reading and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load File Function\n",
    "def load_file(fileName):\n",
    "    data = open(fileName, 'rb')\n",
    "    cifar = pickle.load(data, encoding='bytes')\n",
    "    return cifar\n",
    "\n",
    "#Load Batch Files\n",
    "def get_Batch(fileName):\n",
    "    imageDict = load_file(fileName)\n",
    "    data = imageDict[b'data']\n",
    "    label = imageDict[b'labels']\n",
    "    #Reshap Images\n",
    "    data = data.reshape(10000,3,32,32)\n",
    "    data = data.transpose(0,2,3,1).astype(\"float\")\n",
    "    label = np.array(label)\n",
    "    return data,label\n",
    "\n",
    "#Get All DataSets\n",
    "def get_DataSet(rootPath):\n",
    "    trainX = []\n",
    "    trainY = []\n",
    "    for i in range(1,6):\n",
    "        fileName = os.path.join(rootPath, 'data_batch_%d' % (i, ))\n",
    "        data, label = get_Batch(fileName)\n",
    "        trainX.append(data)\n",
    "        trainY.append(label)\n",
    "    #Combine 6 Training Sets\n",
    "    trainData = np.concatenate(trainX)\n",
    "    trainLabel = np.concatenate(trainY)\n",
    "    #Do the Image Normalization\n",
    "    return trainData/255, trainLabel\n",
    "\n",
    "#Get Test Sets\n",
    "def get_TestSet(rootPath):\n",
    "    testX = []\n",
    "    testY = []\n",
    "    fileName = os.path.join(rootPath, 'test_batch')\n",
    "    data, label = get_Batch(fileName)\n",
    "    testX.append(data)\n",
    "    testY.append(label)  \n",
    "    testData = np.concatenate(testX)\n",
    "    testLabel = np.concatenate(testY)\n",
    "    #Do the Image Normalization\n",
    "    return testData/255, testLabel\n",
    "\n",
    "#Generate the n Dimension Random Sequences\n",
    "def gen_RandomSeque(dim, totalNum, sampleNum):\n",
    "    randomSequences = []\n",
    "    total = range(totalNum)\n",
    "    for i in range(dim):\n",
    "        randomSequences.append(sorted(random.sample(total,sampleNum)))\n",
    "    return randomSequences\n",
    "\n",
    "#Get n Random Fitting Data\n",
    "def get_FitSet(classNum, trainData, trainLabel):\n",
    "    #Number of Each Class is Balanced\n",
    "    totalNum = classNum * 10\n",
    "    randNums = gen_RandomSeque(10,5000,classNum)\n",
    "    #Fitting Data and Label\n",
    "    fitLabel = []\n",
    "    fitData = np.zeros((totalNum,32,32,3))\n",
    "    #Form a Random Sequence with 10k Images\n",
    "    num = 0\n",
    "    count = [0,0,0,0,0,0,0,0,0,0]\n",
    "    for i in range(50000):\n",
    "        label = trainLabel[i]\n",
    "        if(count[label] in randNums[label]):\n",
    "            fitData[num] = trainData[i]\n",
    "            fitLabel.append(label)\n",
    "            num = num + 1\n",
    "        count[label] = count[label] + 1\n",
    "    fitLabel = np.array(fitLabel)\n",
    "    return fitData, fitLabel\n",
    "\n",
    "#Transform the Data Image using Batch Method\n",
    "def PH_Transform(megaSize, batchSize, trainData, model):\n",
    "    previous = 0\n",
    "    features = []\n",
    "    #Image Numbers Per Megabatch\n",
    "    totalNum = trainData.shape[0]\n",
    "    imageNum = int(totalNum / megaSize)\n",
    "    batchNum = int(totalNum / batchSize / megaSize) + 1\n",
    "    #Using Batching Method To Do the Transform\n",
    "    for j in range(megaSize):\n",
    "        feature = []\n",
    "        for i in range(1,batchNum):\n",
    "            trainBatch = trainData[previous:j*imageNum+i*batchSize]\n",
    "            featureBatch = model.transform(trainBatch)\n",
    "            previous = j * imageNum + i * batchSize\n",
    "            if(i == 1):\n",
    "                feature = featureBatch\n",
    "            else:\n",
    "                feature[0] = np.concatenate((feature[0],featureBatch[0]))\n",
    "                feature[1] = np.concatenate((feature[1],featureBatch[1]))\n",
    "                feature[2] = np.concatenate((feature[2],featureBatch[2]))\n",
    "        #print(\"Mega Batch No.\", j)\n",
    "        features.append(feature)\n",
    "    print(\"Transform Done.\")\n",
    "    #Max Pooling Operations\n",
    "    for i in range(megaSize):\n",
    "        features[i][0] = block_reduce(np.ascontiguousarray(features[i][0]), (1, 2, 2, 1), np.max)\n",
    "        features[i][1] = block_reduce(np.ascontiguousarray(features[i][1]), (1, 2, 2, 1), np.max)\n",
    "        features[i][2] = block_reduce(np.ascontiguousarray(features[i][2]), (1, 2, 2, 1), np.max)\n",
    "        #print('Max Pooled Mega Batch', i)\n",
    "    print(\"Max Pooling Done.\")\n",
    "    return features\n",
    "\n",
    "#Merge the Mega Data Batch Together\n",
    "def merge_Features(features, megaNum):\n",
    "    #Merge into 5 Mega Batch 10k\n",
    "    feature_merge = []\n",
    "    batchNum = int(megaNum / 5)\n",
    "    for i in range(5):\n",
    "        feature_temp = []\n",
    "        for j in range(batchNum):\n",
    "            if(j==0):\n",
    "                feature_temp = features[i*batchNum+j]\n",
    "            else:\n",
    "                feature_temp[0] = np.concatenate((feature_temp[0],features[i*batchNum+j][0]))\n",
    "                feature_temp[1] = np.concatenate((feature_temp[1],features[i*batchNum+j][1]))\n",
    "                feature_temp[2] = np.concatenate((feature_temp[2],features[i*batchNum+j][2]))\n",
    "        #print(\"10K Images Merge Done\")\n",
    "        feature_merge.append(feature_temp)\n",
    "    #Merge All Mega Batches 50k\n",
    "    feature_layer1 = np.concatenate((feature_merge[0][0],feature_merge[1][0], \n",
    "                                          feature_merge[2][0],feature_merge[3][0],feature_merge[4][0]))\n",
    "    feature_layer2 = np.concatenate((feature_merge[0][1],feature_merge[1][1], \n",
    "                                          feature_merge[2][1],feature_merge[3][1],feature_merge[4][1]))\n",
    "    feature_layer3 = np.concatenate((feature_merge[0][2],feature_merge[1][2], \n",
    "                                          feature_merge[2][2],feature_merge[3][2],feature_merge[4][2]))\n",
    "    print(\"Features Merge Done\")\n",
    "    #Features Reshape\n",
    "    dataNum = feature_layer1.shape[0]\n",
    "    feature_layer1 = feature_layer1.reshape(dataNum, -1)\n",
    "    feature_layer2 = feature_layer2.reshape(dataNum, -1)\n",
    "    feature_layer3 = feature_layer3.reshape(dataNum, -1)\n",
    "    print(\"Features Reshape Done\")\n",
    "    #Print the Features Shape\n",
    "    print(feature_layer1.shape,feature_layer2.shape,feature_layer3.shape)\n",
    "    return feature_layer1, feature_layer2, feature_layer3\n",
    "\n",
    "#Cross Entropy Calculation for Single layer\n",
    "def cal_CE_layer(features, trainLabel):\n",
    "    kernelNum = features.shape[1]\n",
    "    #Compute the Cross_Entropy of Layer 1\n",
    "    CE = []\n",
    "    for i in range(kernelNum):\n",
    "        tempData = features[:,i]\n",
    "        tempData = tempData.reshape((len(tempData), -1))\n",
    "        ce = Cross_Entropy(num_class=10, num_bin=10)\n",
    "        ce_value = ce.KMeans_Cross_Entropy(tempData, trainLabel)\n",
    "        CE.append(ce_value)\n",
    "    return CE\n",
    "\n",
    "#Cross Entropy Calculation for Each Layer\n",
    "def cal_CE(features1, features2, features3, trainLabel):\n",
    "    CE1 = cal_CE_layer(features1, trainLabel)\n",
    "    CE2 = cal_CE_layer(features2, trainLabel)\n",
    "    CE3 = cal_CE_layer(features3, trainLabel)\n",
    "    print(len(CE1), len(CE2), len(CE3))\n",
    "    return CE1, CE2, CE3\n",
    "\n",
    "#Select Features Indexes for Single CE Array\n",
    "def slct_Indexes_layer(CE, N):\n",
    "    num = len(CE)\n",
    "    index = range(num)\n",
    "    Z = sorted(zip(CE, index))\n",
    "    res, index = zip(*Z)\n",
    "    res = list(res[:N])\n",
    "    index = list(index[:N])\n",
    "    return res, index\n",
    "\n",
    "#Select Features Indexes Method ratio = alpha\n",
    "def slct_Indexes(CE1, CE2, CE3, N):\n",
    "    indexes = []\n",
    "    slctdCE = []\n",
    "    slctdCE1, index1 = slct_Indexes_layer(CE1, N)\n",
    "    slctdCE2, index2 = slct_Indexes_layer(CE2, N)\n",
    "    slctdCE3, index3 = slct_Indexes_layer(CE3, N)\n",
    "    indexes.append(index1)\n",
    "    indexes.append(index2)\n",
    "    indexes.append(index3)\n",
    "    slctdCE.append(slctdCE1)\n",
    "    slctdCE.append(slctdCE2)\n",
    "    slctdCE.append(slctdCE3)\n",
    "    return slctdCE, indexes\n",
    "\n",
    "#Select Features for Single Layer\n",
    "def slct_Features_layer(features, indexes):\n",
    "    dataNum = features.shape[0]\n",
    "    featureNum = len(indexes)\n",
    "    slctdFeature = np.zeros(shape=(dataNum, featureNum))\n",
    "    #print(slctdFeature.shape)\n",
    "    for i in range(featureNum):\n",
    "        slctdFeature[:,i] = features[:, indexes[i]]\n",
    "    return slctdFeature\n",
    "\n",
    "#Select Features based on Indexes List\n",
    "def slct_Features(features1, features2, features3, indexes):\n",
    "    slctdFeatures = []\n",
    "    slctdFeature1 = slct_Features_layer(features1, indexes[0])\n",
    "    slctdFeature2 = slct_Features_layer(features2, indexes[1])\n",
    "    slctdFeature3 = slct_Features_layer(features3, indexes[2])\n",
    "    slctdFeatures.append(slctdFeature1)\n",
    "    slctdFeatures.append(slctdFeature2)\n",
    "    slctdFeatures.append(slctdFeature3)\n",
    "    return slctdFeatures\n",
    "\n",
    "#Features Clustering using LAG\n",
    "def LAG_Fit(slctd_Features_Train, trainLabel):\n",
    "    #lag1 = LAG(encode='distance', num_clusters=[12,12,12,12,12,12,12,12,12,12], alpha=10, learner=myLLSR(onehot=False))\n",
    "    #lag2 = LAG(encode='distance', num_clusters=[12,12,12,12,12,12,12,12,12,12], alpha=10, learner=myLLSR(onehot=False))\n",
    "    #lag3 = LAG(encode='distance', num_clusters=[12,12,12,12,12,12,12,12,12,12], alpha=10, learner=myLLSR(onehot=False))\n",
    "    lag1 = LAG(encode='distance', num_clusters=[5,5,5,5,5,5,5,5,5,5], alpha=10, learner=myLLSR(onehot=False))\n",
    "    lag2 = LAG(encode='distance', num_clusters=[5,5,5,5,5,5,5,5,5,5], alpha=10, learner=myLLSR(onehot=False))\n",
    "    lag3 = LAG(encode='distance', num_clusters=[5,5,5,5,5,5,5,5,5,5], alpha=10, learner=myLLSR(onehot=False))\n",
    "    #Extract Training Features\n",
    "    slctd_Features_Train1 = slctd_Features_Train[0]\n",
    "    slctd_Features_Train2 = slctd_Features_Train[1]\n",
    "    slctd_Features_Train3 = slctd_Features_Train[2]\n",
    "    #Fit the three LAG units\n",
    "    lag1.fit(slctd_Features_Train1, trainLabel)\n",
    "    lag2.fit(slctd_Features_Train2, trainLabel)\n",
    "    lag3.fit(slctd_Features_Train3, trainLabel)\n",
    "    #Get the Transformed Training Features\n",
    "    features_Train_Trans1 = lag1.transform(slctd_Features_Train1)\n",
    "    features_Train_Trans2 = lag2.transform(slctd_Features_Train2)\n",
    "    features_Train_Trans3 = lag3.transform(slctd_Features_Train3)\n",
    "    #Cascade all the LAG features\n",
    "    features_Train_LAG = np.concatenate((features_Train_Trans1,features_Train_Trans2,features_Train_Trans3), axis=1)\n",
    "    return features_Train_LAG, lag1, lag2, lag3\n",
    "\n",
    "#Feature Transform Using Fitted LAGs\n",
    "def LAG_Transform(slctd_Features_Test, lag1, lag2, lag3):\n",
    "    #Extract Testing Features\n",
    "    slctd_Features_Test1 = slctd_Features_Test[0]\n",
    "    slctd_Features_Test2 = slctd_Features_Test[1]\n",
    "    slctd_Features_Test3 = slctd_Features_Test[2]\n",
    "    #Get the Transformed Testing Features\n",
    "    features_Test_Trans1 = lag1.transform(slctd_Features_Test1)\n",
    "    features_Test_Trans2 = lag2.transform(slctd_Features_Test2)\n",
    "    features_Test_Trans3 = lag3.transform(slctd_Features_Test3)\n",
    "    #Cascade all the LAG features\n",
    "    features_Test_LAG = np.concatenate((features_Test_Trans1,features_Test_Trans2,features_Test_Trans3), axis=1)\n",
    "    return features_Test_LAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File Path\n",
    "dataPath = './cifar-10-batches-py'\n",
    "#Get the Training Data\n",
    "trainData, trainLabel = get_DataSet(dataPath)\n",
    "trainLabel_catgory = to_categorical(trainLabel)\n",
    "#Get the Testing Data\n",
    "testData, testLabel = get_TestSet(dataPath)\n",
    "testLabel_category = to_categorical(testLabel)\n",
    "#Get the Fit\n",
    "fitData, fitLabel = get_FitSet(1000,trainData,trainLabel)\n",
    "trainData_4, trainLabel_4 = get_FitSet(1250,trainData,trainLabel)\n",
    "trainData_8, trainLabel_8 = get_FitSet(625,trainData,trainLabel)\n",
    "trainData_16, trainLabel_16 = get_FitSet(312,trainData,trainLabel)\n",
    "trainData_32, trainLabel_32 = get_FitSet(156,trainData,trainLabel)\n",
    "#Choose the Portions\n",
    "data_portion = 1\n",
    "testNum = 10000\n",
    "if (data_portion == 1):\n",
    "    dataNum = 50000\n",
    "    trainDataSet = trainData\n",
    "    trainLabelSet = trainLabel\n",
    "    megaSize = 25\n",
    "    batchSize = 100\n",
    "if (data_portion == 4):\n",
    "    dataNum = 12500\n",
    "    trainDataSet = trainData_4\n",
    "    trainLabelSet = trainLabel_4\n",
    "    megaSize = 25\n",
    "    batchSize = 100\n",
    "if (data_portion == 8):\n",
    "    dataNum = 6250\n",
    "    trainDataSet = trainData_8\n",
    "    trainLabelSet = trainLabel_8\n",
    "    megaSize = 25\n",
    "    batchSize = 50\n",
    "if (data_portion == 16):\n",
    "    dataNum = 3120\n",
    "    trainDataSet = trainData_16\n",
    "    trainLabelSet = trainLabel_16\n",
    "    megaSize = 20\n",
    "    batchSize = 78\n",
    "if (data_portion == 32):\n",
    "    dataNum = 1560\n",
    "    trainDataSet = trainData_32\n",
    "    trainLabelSet = trainLabel_32\n",
    "    megaSize = 20\n",
    "    batchSize = 78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Shrink and Concat Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Callback function for collecting patches and its inverse\n",
    "def Shrink(X, shrinkArg):\n",
    "    num = shrinkArg['num']\n",
    "    win = shrinkArg['win']\n",
    "    stride = shrinkArg['stride']\n",
    "    channel = X.shape[-1]\n",
    "    if(num > 1):\n",
    "        X = block_reduce(X, (1, 2, 2, 1), np.max)\n",
    "    X = view_as_windows(np.ascontiguousarray(X), (1,win,win,channel), (1,stride,stride,channel))\n",
    "    X = X.reshape(X.shape[0], X.shape[1], X.shape[2], -1)\n",
    "    return X\n",
    "\n",
    "# example callback function for how to concate features from different hops\n",
    "def Concat(X, concatArg):\n",
    "    return X\n",
    "\n",
    "# set args\n",
    "SaabArgs = [{'num_AC_kernels':-1, 'needBias':False, 'useDC':True, 'batch':None, 'cw':False}, \n",
    "            {'num_AC_kernels':-1, 'needBias':True, 'useDC':True, 'batch':None, 'cw':True},\n",
    "            {'num_AC_kernels':-1, 'needBias':True, 'useDC':True, 'batch':None, 'cw':True}]\n",
    "shrinkArgs = [{'func':Shrink, 'win':5, 'stride':1, 'num':1}, \n",
    "              {'func':Shrink, 'win':5, 'stride':1, 'num':2},\n",
    "              {'func':Shrink, 'win':5, 'stride':1, 'num':3},]\n",
    "concatArg = {'func':Concat}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Time Starts\n",
    "trainStart = datetime.datetime.now()\n",
    "#PixelHop Fitting\n",
    "print(\"Training the Module 1 of PixelHop\")\n",
    "model = Pixelhop2(depth=3, TH1=0.001, TH2=0.0001, SaabArgs=SaabArgs, shrinkArgs=shrinkArgs, concatArg=concatArg)\n",
    "model.fit(fitData)\n",
    "#Using Batching Method To Do the Transform\n",
    "print(\"Extracting Features of Training Data\")\n",
    "features_train_raw = PH_Transform(megaSize, batchSize, trainDataSet, model)\n",
    "#Merge Training Image Features\n",
    "features_train_layer1, features_train_layer2, features_train_layer3 = merge_Features(features_train_raw, megaSize)\n",
    "#Feature Selection Process\n",
    "print(\"Selecting Features of Training Data\")\n",
    "#Calculate the Cross Entrophy for Each Channel\n",
    "CE1, CE2, CE3 = cal_CE(features_train_layer1, features_train_layer2, features_train_layer3, trainLabelSet)\n",
    "#Select Feature Indexes based on CEs\n",
    "slctdCE, indexes = slct_Indexes(CE1, CE2, CE3, 1000)\n",
    "#Select Training Features based on Indexes List\n",
    "features_train_selected = slct_Features(features_train_layer1, features_train_layer2, features_train_layer3, indexes)\n",
    "#Training Features LAG Fitting and Transform\n",
    "print(\"LAG Features of Training Data\")\n",
    "features_Train_LAG, lag1, lag2, lag3 = LAG_Fit(features_train_selected, trainLabelSet)\n",
    "#Train the SVM\n",
    "scaler = preprocessing.StandardScaler()\n",
    "features_Train_final = scaler.fit_transform(features_Train_LAG)\n",
    "classifier = SVC().fit(features_Train_final, trainLabelSet)\n",
    "#Training Accuracy\n",
    "print('***** Train ACC:', accuracy_score(trainLabelSet, classifier.predict(features_Train_final)))\n",
    "#Training Time Ends\n",
    "trainEnd = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Testing Time Starts\n",
    "testStart = datetime.datetime.now()\n",
    "print(\"Extracting Features of 10k Testing Data\")\n",
    "features_test_raw = PH_Transform(25, 100, testData, model)\n",
    "#Merge 10K Testing Image Features\n",
    "features_test_layer1, features_test_layer2, features_test_layer3 = merge_Features(features_test_raw, 25)\n",
    "#Select Testing Features based on Indexes List\n",
    "features_test_selected = slct_Features(features_test_layer1, features_test_layer2, features_test_layer3, indexes)\n",
    "#Testing Features LAG Transform\n",
    "features_Test_LAG = LAG_Transform(features_test_selected, lag1, lag2, lag3)\n",
    "#Tesing Classifier\n",
    "features_Test_final = scaler.transform(features_Test_LAG)\n",
    "#Testing Accuracy\n",
    "print('***** Test ACC:', accuracy_score(testLabel, classifier.predict(features_Test_final)))\n",
    "#Testning Time Ends\n",
    "testEnd = datetime.datetime.now()\n",
    "#Number of Parameters Of SVM\n",
    "print(classifier.dual_coef_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Time:', trainEnd - trainStart)\n",
    "print('Testing Time:', testEnd - testStart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredict = classifier.predict(features_Test_final)\n",
    "labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "file, im = plt.subplots(figsize = (10, 8))\n",
    "C = confusion_matrix(testLabel, testPredict)\n",
    "C = C / 1000\n",
    "im.set_xlabel('Predicted Label')\n",
    "im.set_ylabel('True Label')\n",
    "sns.heatmap(C, annot=True, ax = im, fmt ='.2f')\n",
    "im.set_title('Normalized Confusion Matrix')\n",
    "im.set_xticklabels(labels, rotation=90)\n",
    "im.set_yticklabels(labels, rotation=0)\n",
    "file.savefig('Confusion Matrix.jpg', dpi=100, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Exemplary Confusion Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errFile, ax = plt.subplots(18,10,squeeze=True, figsize = (100,100))\n",
    "count = 0\n",
    "fname = 'err.jpg'\n",
    "trueLabel = [3,3,3,5,5,2,2,2,2,2,4,4,4,4,8,0,9,1]\n",
    "predLabel = [5,2,6,3,2,0,3,4,5,6,2,3,6,7,0,8,1,9]\n",
    "for i in range(18):\n",
    "    count = 0\n",
    "    index = []\n",
    "    for j in range(10000):\n",
    "        if (testLabel[j] == trueLabel[i] and testPredict[j] == predLabel[i]):\n",
    "            index.append(j)\n",
    "    use = random.sample(index,10)\n",
    "    for j in use:\n",
    "        ax[i][count].imshow(testData[j])\n",
    "        ax[i][count].axes.get_xaxis().set_visible(False)\n",
    "        ax[i][count].axes.get_yaxis().set_visible(False)\n",
    "        count = count + 1\n",
    "errFile.savefig(fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
